<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2025-11-18 Tue 21:52 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Open Source Accelerated Deep Learning</title>
<meta name="author" content="Harley Wiltzer" />
<meta name="generator" content="Org Mode" />
<style type="text/css">
  #content { max-width: 60em; margin: auto; }
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #e6e6e6;
    border-radius: 3px;
    background-color: #f2f2f2;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: auto;
  }
  pre.src:before {
    display: none;
    position: absolute;
    top: -8px;
    right: 12px;
    padding: 3px;
    color: #555;
    background-color: #f2f2f299;
  }
  pre.src:hover:before { display: inline; margin-top: 14px;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-authinfo::before { content: 'Authinfo'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { }
</style>
<link rel="stylesheet" type="text/css" href="../../assets/css/pandoc.css" />
<link rel="stylesheet" type="text/css" href="../../assets/css/navbar.css" />
</head>
<body>
<div id="content" class="content">
<div class="header-navbar" id="org3fd1eb4">
<ul class="org-ul">
<li><a href="../.././index.html">Home</a></li>
<li><a href="../.././about">About</a></li>
</ul>
<hr />

</div>
<div id="outline-container-org61a7ddb" class="outline-2">
<h2 id="org61a7ddb">Open Source Accelerated Deep Learning</h2>
<div class="outline-text-2" id="text-org61a7ddb">
<p>
<i>Harley Wiltzer, December 2020</i>
</p>

<p>
In 2016, following the first year of my undergrad, I built a PC. I try
to use open source software, particularly <i>free</i> open source software
("free" as in "freedom", not as in beer, of course), so naturally I
bought a fancy new AMD Radeon RX480 GPU. This was also around the time
that the open source AMDGPU drivers were gaining some popularity among
the Linux community, as performance was beginning to rival that of the
properietary NVidia drivers. Then a little while later I became
fascinated by machine learning and had to face the hard "reality" that
NVidia's CUDA, supported by the canonical deep learning frameworks,
leads to massive performance gains for tensor computations often used
in deep learning. Since CUDA is only compatible with NVidia cards, I
began to wonder if my Radeon card was an expensive mistake.
</p>

<p>
Thankfully, this turned out to not be the case. In fact, I'm more
proud of my Radeon purchase now than I ever was. I very recently
learned about the open source ROCm (Radeon Open Compute) software by
AMD that provides GPU acceleration for such tensor computations with
AMD cards, as well as some other neat tools that help unify
acceleration for various GPU architectures, which I believe the deep
learning community can make great use of. I had head of ROCm a few
years ago, but it appeared back then that ROCm wasn't very functional
or robust, so I naively disregarded it. A couple of weeks ago,
inspired by a Reddit post, I decided to dive deep into ROCm and I was
determined to set it up on my PC.
</p>
</div>
<div id="outline-container-orgd43ce78" class="outline-3">
<h3 id="orgd43ce78">Building from Source, and a Rant About Package Managers</h3>
<div class="outline-text-3" id="text-orgd43ce78">
<p>
Installing ROCm is&#x2026; challenging. I don't mean to blame the
developers, ROCm is an extremely complex project with many, many
distinct components that interact with each other, and this naturally
causes build challenges. Especially given that this is all being
provided for free, a challenging build is a price that I'm willing to
pay. What's less acceptable to me, however, is the absurdity of AMD's
documentation for the installation of ROCm. Again, ROCm is composed of
a plethora of distinct packages, so one would expect there to be a
coherent build guide, or even just a diagram to show dependencies, but
it's much, much worse than that. AMD only provides support for
Ubuntu/Debian, so the only actual guides revolve around building
Ubuntu Docker containers that source ROCm binaries from AMD's
repository. This is disgusting.
</p>
</div>
<div id="outline-container-orgf9a73c6" class="outline-4">
<h4 id="orgf9a73c6">The Rant about Package Managers</h4>
<div class="outline-text-4" id="text-orgf9a73c6">
<p>
Some years ago, I was pointed to <a href="http://michael.orlitzky.com/articles/motherfuckers_need_package_management.xhtml">this wonderful blog post</a> (please
excuse the profanity) by Michael Orlitzky. While it is blatantly
eccentric, this article has since been perpetually at the back of my
mind, because the message it conveys is very bold, makes some great
points, and is portrayed in a humorous manner. The idea is that people
tend to be too afraid and/or too lazy to either build or use proper
package managers. For instance, the trend these days it to make
language-specific package managers (<code>pip</code>, <code>cargo</code>, <code>cabal</code>, <code>node</code>,
&#x2026;), platform-dependent package managers (<code>apt</code>), and package
managers that only know how to handle binaries (<code>apt</code>, <code>pacman</code>). And
this is all garbage, especially the first two categories. With
language-specific package managers, users end up with a boatload of
disjoint package managers that they almost surely are not keeping up
to date, and by construction they do not cohesively manage
dependencies. Consequently people use things like <code>virtualenv</code> and
<code>conda</code>, which basically says "rather than worry about dependencies,
why not just put each project in an isolated box with its own
dependencies". This is stupid because now you have several copies of
all these dependencies and you're still not managing dependencies
properly, in fact you're neglecting the issue of dependency hell
altogether. And Docker containers are basically the same idea, but
rather than bundling some python code with some libraries, it's an
improvement since it bundles an entire operating system! Let me make
it very clear that the previous sentence was sarcastic, because this
is an absolutely atrocious idea for similar reasons (for the record, I
recognize that Docker does make sense in some settings, it just
doesn't make any sense to depend on it for projects you're building on
your PC).
</p>

<p>
The author's suggested solution is pretty bold. He claims (correctly)
that the <a href="https://gentoo.org">Gentoo Linux</a> package manager, <code>portage</code>, satisfies all of the
desired properties of package managers. Moreover, since it's
cross-platform, he says every software developer should distribute
software for <code>portage</code>. Note that the author is a Gentoo developer, so
there is clearly some bias. But the more I think about it, the more it
starts to make sense to me. And my journey to install ROCm only
cemented this idea further.
</p>
</div>
</div>
<div id="outline-container-org9d8a890" class="outline-4">
<h4 id="org9d8a890">The Need to Build from Source</h4>
<div class="outline-text-4" id="text-org9d8a890">
<p>
So in the previous section, I mentioned how the ability for a package
manager to build software from source is very desirable. Until I
started building software from source frequently, I didn't realize
that lots of software is distributed with lots of configurable
parameters that actually affect the binary that is produced. For
instance, Vim provides a massive selection of features, many of which
I have no use for. By configuring the build, I can choose to simply
not build in these features, resulting in a more lightweight
binary. More severely, building ROCm (and the PyTorch with ROCm
support) allow you to configure the build depending on your GPU
architecture. Quoting Michael Orlitzky's article, by simply
downloading a binary package, you are assuming that whoever built it
for you has to
</p>

<blockquote>
<ol class="org-ol">
<li>know that your architecture exists; and</li>
<li>give a s*** about you; and</li>
<li>find a computer like yours to test it</li>
</ol>

<p>
Usually one of these criteria is not met (guess which one).
</p>
</blockquote>

<p>
Indeed, at least one of these criteria was not met when I tried
ROCm with those pre-built Docker containers, as the ROCm system was
built without support for my GPU. Also containers suck, as I explained
before, so I set out to build ROCm myself. However, since it's
expected that everyone will use pre-made containers, very little
effort was made to make the process of building from source a smooth
one.
</p>
</div>
</div>
<div id="outline-container-org27c339c" class="outline-4">
<h4 id="org27c339c">Install Gentoo</h4>
<div class="outline-text-4" id="text-org27c339c">
<p>
So what's the solution to this problem? Michael Orlitzky suggests that
all software developers should distribute software as <i>ebuilds</i>, which
are essentially shell scripts that tell Gentoo's package manager
<code>portage</code> how to configure the source, build the software, and manage
its dependencies. And after this experience, I have to say that I
kinda agree with this notion. I mean, I acknowledge that this is an
unrealistic proposal, but there are some absolutely awesome advantages
to using <code>portage</code> that are just intrinsically great, and I can't
imagine any cogent argument that suggests otherwise. Here's some
reasons why:
</p>
<ol class="org-ol">
<li><code>portage</code> goes beyond just managing dependencies, it can even
figure out dependency satisfaction due to those build configuration
options I described above.</li>
<li>Since the dependencies are managed so nicely, the ebuilds
themselves make up a build guide &#x2013; that is, you don't need any
instructions beyond the ebuilds to get the build up and running.</li>
<li>The Gentoo developers have developed useful standards for
implementing a vast array of common operations, which significantly
reduces boilerplate and requisite mental capacity.</li>
<li>The Gentoo developers have also developed automated QA to reinforce
good practices. This includes a standard for installing binaries,
headers, libraries, and anything else; this alone should
substantially reduce build failures.</li>
</ol>
<p>
This list is not comprehensive, by the way.
</p>

<p>
Thankfully, I've been using Gentoo for a few year already &#x2013; I didn't
install it for the sole purpose of getting ROCm to work properly and
to write this post. But you don't actually need to install Gentoo to
use <code>portage</code> (it has cross-platform support!), rather, you might
prefer simply using <a href="https://wiki.gentoo.org/wiki/Project:Prefix">Gentoo Prefix</a>. While this does in fact add a
package manager to your system, it is at least an extremely robust
package manager that can suit all of your needs.
</p>
</div>
</div>
</div>
<div id="outline-container-org383e637" class="outline-3">
<h3 id="org383e637">Installing ROCm with Portage</h3>
<div class="outline-text-3" id="text-org383e637">
<p>
Over the past little while, I've been tweaking ebuilds so I can get a
working build of ROCm. This job was made much easier than it could've
been, thanks to the very nice set of ebuilds maintained by <a href="https://github.com/justXi">justXi</a> for
ROCm. Unforunately, the versions of ROCm that they maintain are too
recent, and they do not support my GPU. Thus, I worked on making
functioning ebuilds for an older version (3.5.1), which both supports
older GPUs and is supported by Tensorflow and PyTorch. My <a href="https://github.com/harwiltz/rocm">overlay</a> is
available on GitHub if you'd like to follow along.
</p>

<p>
Installing the ROCm core is done as follows:
</p>
<ol class="org-ol">
<li>Build the <code>amd-rocm-meta</code> package:</li>
</ol>
<div class="org-src-container">
<pre class="src src-bash">$ sudo emerge --ask --verbose <span style="color: #50a14f;">"=sci-libs/amd-rocm-meta-3.5.1"</span>
</pre>
</div>

<p>
And that's all. Well I guess that's a bit of an exaggeration since
you'll need to set up an overlay and perhaps some configuration first,
but these tasks just become routine as you get used to Portage. I'll
outline the steps explicitly.
</p>
</div>
<div id="outline-container-orgf577e19" class="outline-4">
<h4 id="orgf577e19">1. Set up an overlay</h4>
<div class="outline-text-4" id="text-orgf577e19">
<p>
In order for Portage to "register" a third party set of packages (or
ebuilds, more precisely), you'll need to set up an <i>overlay</i>. Gentoo
provides a nice tool called <code>layman</code> to help with this, however I find
it's actually easier to set it up manually. I'll be referring to the
variable <code>${EPREFIX}</code> to account for a Prefix setup as opposed to an
actual Gentoo system. On Gentoo, <code>${EPREFIX}</code> simply refers to the
root directory. With that out of the way, let's begin.
</p>

<p>
Start by cloning my repo and putting it in a nice location. Also make
yourself part of the <code>portage</code> group to make editing ebuilds much
easier.
</p>

<div class="org-src-container">
<pre class="src src-bash">$ sudo usermod -a -G portage ${<span style="color: #8b4513;">LOGNAME</span>}
$ cd ${<span style="color: #8b4513;">EPREFIX</span>}/var/db/repos
$ git clone https://github.com/harwiltz/rocm
$ chown -R ${<span style="color: #8b4513;">LOGNAME</span>}:portage rocm
</pre>
</div>

<div class="org-src-container">
<label class="org-src-name"><span class="listing-number">Listing 1: </span><code>${EPREFIX}/etc/portage/repos.conf/rocm.conf</code></label><pre class="src src-bash">[rocm]
location = ${<span style="color: #8b4513;">EPREFIX</span>}/var/db/repos/rocm
</pre>
</div>

<p>
Now the overlay is ready to go.
</p>
</div>
</div>
<div id="outline-container-org70ed9ba" class="outline-4">
<h4 id="org70ed9ba">2. Configure Portage</h4>
<div class="outline-text-4" id="text-org70ed9ba">
<p>
I guess this part is optional if you declare the environment variables
manually when installing <code>amd-rocm-meta</code>. But I think this is a nicer
solution.
</p>

<div class="org-src-container">
<label class="org-src-name"><span class="listing-number">Listing 2: </span><code>${EPREFIX}/etc/portage/make.conf</code></label><pre class="src src-bash">...
<span style="color: #8b4513;">HIP_PLATFORM</span>=rocclr
<span style="color: #8b4513;">MAKEOPTS</span>=<span style="color: #50a14f;">"-j4"</span>
</pre>
</div>
<p>
The <code>MAKEOPTS</code> variable determines how many parallel processes can be
run during the build, people often say to set this to <code>n_cpu_cores +
1</code>. I tend to set it a little lower than that so I can use my machine
while building stuff in the background.
</p>

<p>
Now, before packages are tested thoroughly they are <i>keyworded</i> by the
CPU architectures for which they're still being tested. We need to
tell Portage to accept these packages. Usually I just run my <code>emerge</code>
commands and update my configuration as it tells me to, because it
could be a little difficult/tedious to figure them all out <i>a
priori</i>. But here's the relevant configuration that works for me.
</p>

<div class="org-src-container">
<label class="org-src-name"><span class="listing-number">Listing 3: </span><code>${EPREFIX}/etc/portage/package.accept_keywords/rocm</code></label><pre class="src src-bash">=sci-libs/pytorch-1.6* ~amd64
=dev-libs/half-2.1.0 ~amd64
=dev-libs/rccl-3.5.0 ~amd64
=dev-libs/rocclr-3.5.0 ~amd64
=dev-libs/rocm-comgr-3.5.0 ~amd64
=dev-libs/rocm-device-libs-3.5.0 ~amd64
=dev-libs/rocm-opencl-runtime-3.5.0 ~amd64
=dev-libs/rocm-smi-lib-3.5.0 ~amd64
=dev-libs/rocr-runtime-3.5.0 ~amd64
=dev-libs/roct-thunk-interface-3.5.0 ~amd64
=dev-util/amd-rocm-meta-3.5.1 ~amd64
=dev-util/rocminfo-3.5.0 ~amd64
=dev-util/rocm-clang-ocl-3.5.0 ~amd64
=dev-util/rocm-cmake-3.5.0 ~amd64
=dev-util/rocm-smi-3.5.0 ~amd64
=dev-util/roctracer-3.5.0 ~amd64
=sci-libs/hipBLAS-3.5.0 ~amd64
=sci-libs/hipCUB-3.5.0 ~amd64
=sci-libs/hipSPARSE-3.5.0 ~amd64
=sci-libs/miopen-3.5.0 ~amd64
=sci-libs/rocFFT-3.5.0 ~amd64
=sci-libs/rocBLAS-3.5.0 ~amd64
=sci-libs/rocPRIM-3.5.1 ~amd64
=sci-libs/rocRAND-3.5.0 ~amd64
=sci-libs/rocSOLVER-3.5.0 ~amd64
=sci-libs/rocSPARSE-3.5.0 ~amd64
=sci-libs/rocThrust-3.5.0 ~amd64
=sys-devel/hcc-3.3.0 ~amd64 <span style="color: #a0a1a7; font-weight: bold;"># </span><span style="color: #a0a1a7;">Deprecated, no newer releases
</span>=sys-devel/hip-3.5.1 ~amd64
=sys-devel/llvm-roc-3.5.1 ~amd64
dev-python/CppHeaderParser ~amd64
</pre>
</div>

<p>
Note that the file above can be named arbitrarily (as long as it's
under <code>${EPREFIX}/etc/portage/package.accept_keywords</code>).
</p>

<p>
Finally, if you plan on installing deep learning libraries, I
recommend also adding the following,
</p>

<div class="org-src-container">
<label class="org-src-name"><span class="listing-number">Listing 4: </span><code>${EPREFIX}/etc/portage/package.use/rocm</code></label><pre class="src src-bash">sci-libs/amd-rocm-meta deeplearning
</pre>
</div>

<p>
Like the previous example, this file can be named arbitrarily as long
as it's in the right directory. Here, we are setting the
<code>deeplearning</code> USE flag to the affirmative for the <code>amd-rocm-meta</code>
package, which will activate some more dependencies.
</p>

<p>
Now we're ready to start the build!
</p>
</div>
</div>
<div id="outline-container-orgdc156e5" class="outline-4">
<h4 id="orgdc156e5">3. Building ROCm</h4>
<div class="outline-text-4" id="text-orgdc156e5">
<p>
We can now run the command I foreshadowed earlier,
</p>

<div class="org-src-container">
<pre class="src src-bash">$ sudo emerge -av amd-rocm-meta
</pre>
</div>

<p>
When this build completes, you'll be all set to install PyTorch or
Tensorflow. I believe these are the only two major ML frameworks that
have support for ROCm, although to be honest I haven't looked very
far.
</p>
</div>
<div id="outline-container-org10b44c5" class="outline-5">
<h5 id="org10b44c5">Potential build issues</h5>
<div class="outline-text-5" id="text-org10b44c5">
<p>
Given the complexity of this system and the intensity of some of the
builds, the build of <code>amd-rocm-meta</code> might not succeed all the way
through on the first go. Thankfully, however, with the state of the
ebuilds at the time I'm writing this post, I only ran into one problem
that occurred on a few of the dependencies. Some of these builds, such
as <code>rocFFT</code>, use <b>lots</b> of memory and compute. I've seen it suggested
from a few sources to have 16G of memory available for the build, and
I don't think that's a conservative estimate. In fact in one of the
ebuilds in <a href="https://github.com/justxi/rocm">justXi's overlay</a>, there is a warning that 28G of memory
should be available. My machine has "only" 16G of memory in total, so
I figured I might be safe (especially given that my Gentoo setup is
very lean on RAM usage). However, occasionally I would run into build
errors that turned out to be due to insufficient memory.
</p>

<p>
The good news is that, at least in my experience, this problem can
actually be mitigated. In particular, if you experience a build error
that appears very odd and that you can't explain, it could very well
be due to some memory corruption, but you can try the following,
</p>

<div class="org-src-container">
<pre class="src src-bash">$ sudo <span style="color: #8b4513;">MAKEOPTS</span>=<span style="color: #50a14f;">"-j2"</span> emerge -v1 &lt;package&gt;
$ sudo emerge -v amd-rocm-meta
</pre>
</div>

<p>
where <code>&lt;package&gt;</code> is the package whose build failed. The "2" in
<code>"-j2"</code> can really be replaced by any number less than the one you
specified in <code>make.conf</code> earlier. Note that it would not behoove you
to run
</p>

<div class="org-src-container">
<pre class="src src-bash">$ sudo <span style="color: #8b4513;">MAKEOPTS</span>=<span style="color: #50a14f;">"-j2"</span> emerge -v amd-rocm-meta
</pre>
</div>

<p>
since I suspect that many dependencies should build successfully with
more processes.
</p>
</div>
</div>
</div>
<div id="outline-container-org4fc260e" class="outline-4">
<h4 id="org4fc260e">4. Testing the ROCm setup</h4>
<div class="outline-text-4" id="text-org4fc260e">
<p>
As a quick sanity check, execute the following,
</p>
<div class="org-src-container">
<pre class="src src-bash">$ ${<span style="color: #8b4513;">EPREFIX</span>}/usr/lib/rocm_agent_enumerator
gfx000
gfx803
</pre>
</div>
<p>
This should list codes that identify you're GPU architecture. The
<code>gfx000</code> output corresponds to a CPU, so if that's your only output,
something went wrong.
</p>

<p>
You can also try running <code>${EPREFIX}/usr/bin/rocminfo</code> and check if it
lists correct information about your GPU.
</p>
</div>
</div>
</div>
<div id="outline-container-org979aaae" class="outline-3">
<h3 id="org979aaae">Installing Deep Learning Libraries</h3>
<div class="outline-text-3" id="text-org979aaae">
<p>
Assuming everyting above was successful, you should be able to install
Tensorflow or PyTorch. I'll go over the PyTorch install since that's
my preferred library. However I did quickly test that Tensorflow works
with this ROCm setup. For PyTorch, begin by running
</p>

<div class="org-src-container">
<pre class="src src-bash">$ sudo <span style="color: #8b4513;">MAX_JOBS</span>=4 emerge -av <span style="color: #50a14f;">"=sci-libs/pytorch-1.6.0-r2::rocm"</span>
</pre>
</div>

<p>
The <code>MAX_JOBS</code> variable like before controls how many concurrent
processes can be run, and should be tuned if you get some weird build
failures. If you have a huge amount of memory available, you don't
need to set this variable. Unfortunately, before using PyTorch, you
currently need to manually make some symlinks since the <code>torch</code> module
is hardcoded to look for its shared objects under <code>/usr/lib</code>, which
Gentoo reserves for 32-bit binaries. Portage complains when you try to
install 64-bit binaries there, so the shared objects are installed under
<code>/usr/lib64</code>. So, all we need to do is
</p>

<div class="org-src-container">
<pre class="src src-bash">$ sudo mkdir -p /usr/lib/python3.7/site-packages/torch
$ sudo ln -s ${<span style="color: #8b4513;">EPREFIX</span>}/usr/lib64/python3.7/site-packages/torch/lib <span style="color: #50a14f;">\</span>
             /usr/lib/python3.7/site-packages/torch/lib
</pre>
</div>

<p>
And now we can experience the moment we've all been waiting for:
</p>

<div class="org-src-container">
<pre class="src src-python">$ python
&gt;&gt;&gt; <span style="color: #a626a4;">import</span> torch
&gt;&gt;&gt; torch.cuda.is_available()
<span style="font-weight: bold; text-decoration: underline;">True</span>
</pre>
</div>

<p>
You might see some "error messages" that complain about missing files
&#x2013; these files are indeed missing, as AMD has not yet made them open
source. However, they are not needed to make good use of your AMD GPU
for deep learning. You might also find it odd that it says that CUDA
is available even though we're not using CUDA. This is expected
behavior &#x2013; AMD provides a script to translate all CUDA code to HIP,
which is an abstraction layer made by AMD that supports both AMD and
NVidia backends (isn't this awesome?). Everything is still referred to
by CUDA to make it easy to reuse code for the time being.
</p>

<p>
I ran a quick toy benchmark to get an idea of how much faster some
computations would be on the GPU. Here's the corresponding (pseudo)
code:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #a626a4;">import</span> torch

<span style="color: #a0a1a7; font-weight: bold;"># </span><span style="color: #a0a1a7;">Repeat 10000 times
</span><span style="color: #a0a1a7; font-weight: bold;"># </span><span style="color: #a0a1a7;">Record time of matmul for each
</span><span style="color: #8b4513;">a</span> = torch.uniform(size=(3, 78, 78))
<span style="color: #8b4513;">b</span> = torch.uniform(size=(3, 78, 78))
<span style="color: #8b4513;">a_gpu</span> = a.cuda()
<span style="color: #8b4513;">b_gpu</span> = b.cuda()

torch.matmul(a, b)
torch.matmul(a_gpu, b_gpu)
</pre>
</div>

<p>
The results were convincingly in the GPU's favor:
</p>

<img src="/assets/img/amdgpu_benchmark.png"></img>

<p>
<i>By the way, the "feature image" for this post depicts <a href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwjBhrb6jLHtAhVCu1kKHRTZARYQFjACegQIAxAC&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3Di2lhwb_OckQ&amp;usg=AOvVaw2UgoF2uFi6szUxMTZwUUAD">Linus Torvalds
flipping off NVidia</a>.</i>
</p>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: Harley Wiltzer</p>
<p class="date">Created: 2025-11-18 Tue 21:52</p>
</div>
</body>
</html>
